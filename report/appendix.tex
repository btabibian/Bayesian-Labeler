\section{ Derivation of Bayesian Inference Engine}
Every possible action of the user can be shown as a vector $X_{act}$ in which:
\begin{itemize}
\item $act \in \{open, save, edit, add\}$.
\item every element of $X_{act}$ can take values $0,1$.
\item $\sum X_{act} =1$. 
\end{itemize}

possible actions are also one of $(open, save, edit, add)$ and therefore the size of $X_{act} = 4$. An example vector is:
\[ X_{open}=(0,0,0,1)^T \]
Which is interpreted as if user chooses action open, the next action will be to add a label to the image.


Probability distribution over all possible actions can be modelled as a \textit{Multinomial Distribution}:

\[
Mult(m_1,m_2,...,m_K|\mu,N)={N \choose m_1 m_2 ... m_K}\prod_{k=1}^K \mu_{k}^{x_k}
\]

where $\mu=(\mu_1,\mu_2,...,\mu_K)^T$. Finding the maximum likelihood solution for parameter $\mu$ will give us:

\[
\mu_k^{ML}={m_k \over N} 
\]

We use this result to approximate probability for the next action to be chosen. It is natural to think about this result as averaging over the observations.
To take one step further and introduce priors to the distribution we use \textit{Dirichlet distribution} which is the conjugate prior of the multinomial distribution.
\[
Dir(\mu|\alpha)={\Gamma(\alpha_0) \over \Gamma(\alpha_1)...\Gamma(\alpha_K)}\prod_{k=1}^K \mu_{k}^{\alpha_k -1}
\]
Conjugate priors have certain characteristics which makes it easy to compute the posterior of the distribution that is constructed by multiplying the likelihood function by the prior.  
\[
p(D|\mu)=\prod_{k=1}^K {\mu_{k}^{m_k}}\]

\[p(\mu| D,\alpha) \propto p(D|\mu)  p(\mu|\alpha)\]

\[p(\mu|D, \alpha)={\Gamma(\alpha_0+N) \over {\Gamma(\alpha_1+m_1)...\Gamma(\alpha_K+m_K)}}\prod_{k=1}^K \mu_{k}^{\alpha_k+m_k-1}\]

Marginalizing over $\mu$ we get 

\[p(x=add|D)=\int_0^1 p(x=add|\mu)p(\mu|D) \mathrm{d}\mu\]
\[p(x=add|D)=\int_0^1 \mu p(\mu|D)\mathrm{d}\mu = \mathbb{E}[\mu|D]\]

Last line is equal to:

\[\mathbb{E}[\mu|D]={m_k+\alpha_k \over {N+\alpha_0}} \]

The result, just like earlier, can be interpreted as considering priors as imaginary observations.

After calculating the $\mu$ of every action the suggestion is produced by using the Inverse Transform\cite{Devroye1986} method and a uniform distribution.

In conclusion, the resulting model ,even though simple ,is tractable and produces expected results which are robust.

For further information the reader is referred to \cite{Bishop2007}.
